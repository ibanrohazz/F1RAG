{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHOp9Ls0FwZ3"
   },
   "source": [
    "Load the Formula 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoXhWvsHFe8j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# (Adjust paths if needed)\n",
    "races = pd.read_csv('races.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "results = pd.read_csv('results.csv')\n",
    "constructors = pd.read_csv('constructors.csv')\n",
    "\n",
    "print(\"Loaded:\", len(races), \"races,\", len(drivers), \"drivers,\", len(results), \"results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy4Z1o01Fx44"
   },
   "source": [
    "Build the F1 Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zB24yq19FmeM"
   },
   "outputs": [],
   "source": [
    "f1_data = results.merge(races, on='raceId')\n",
    "f1_data = f1_data.merge(drivers, on='driverId')\n",
    "f1_data = f1_data.merge(constructors, on='constructorId')\n",
    "\n",
    "# Only winning results\n",
    "winners = f1_data[f1_data['positionOrder'] == 1]\n",
    "\n",
    "# Create rich fact descriptions\n",
    "winners['fact'] = winners.apply(lambda row: f\"In {row['year']}, {row['forename']} {row['surname']} won the {row['name']} driving for {row['name_y']}.\", axis=1)\n",
    "f1_facts = winners[['fact']].reset_index(drop=True)\n",
    "\n",
    "print(\"Sample Fact:\", f1_facts.iloc[0]['fact'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t07ZLcGYF1Jf"
   },
   "source": [
    "Embed the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fposwmHpFoYi"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Embedding function\n",
    "def compute_embeddings(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0]\n",
    "            embeddings = F.normalize(embeddings, dim=1)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Compute\n",
    "fact_embeddings = compute_embeddings(f1_facts['fact'].tolist())\n",
    "print(\"Embedded\", fact_embeddings.shape[0], \"facts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFwdKq9CF3yA"
   },
   "source": [
    "Define the FAISS Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zXqb9s6FqA0"
   },
   "outputs": [],
   "source": [
    "class F1FAISSRetriever:\n",
    "    def __init__(self, facts, embeddings):\n",
    "        self.facts = facts\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings.numpy())\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            output = bert(**inputs)\n",
    "            query_emb = output.last_hidden_state[:, 0]\n",
    "            query_emb = F.normalize(query_emb, dim=1)\n",
    "\n",
    "        D, I = self.index.search(query_emb.cpu().numpy(), k)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx != -1:\n",
    "                results.append((self.facts[idx], score))\n",
    "        return results\n",
    "\n",
    "retriever = F1FAISSRetriever(f1_facts['fact'].tolist(), fact_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoKrYm-2F5k9"
   },
   "source": [
    "Build the Conversational Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhBW3YvwFrZT"
   },
   "outputs": [],
   "source": [
    "class F1Chatbot:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.chat_history = []\n",
    "\n",
    "    def chat(self, query, top_k=3):\n",
    "        self.chat_history.append({\"user\": query})\n",
    "        results = self.retriever.retrieve(query, k=top_k)\n",
    "        answer = \"\\n\".join([f\"- {fact}\" for fact, score in results])\n",
    "        self.chat_history.append({\"bot\": answer})\n",
    "\n",
    "        print(f\"\\nUser: {query}\")\n",
    "        print(f\"Bot:\\n{answer}\\n\")\n",
    "\n",
    "    def show_history(self):\n",
    "        for turn in self.chat_history:\n",
    "            for speaker, text in turn.items():\n",
    "                print(f\"{speaker.capitalize()}: {text}\\n\")\n",
    "\n",
    "f1_chatbot = F1Chatbot(retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E136cSx_F6wy"
   },
   "source": [
    "Example Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcCZxGzlFs6P"
   },
   "outputs": [],
   "source": [
    "f1_chatbot.chat(\"Who won the Monaco Grand Prix in 2019?\")\n",
    "f1_chatbot.chat(\"How about Silverstone in 2014?\")\n",
    "f1_chatbot.chat(\"Who was champion in 2008?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcSu9R5WFt1k"
   },
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "# faiss.write_index(retriever.index, 'f1_facts.index')\n",
    "\n",
    "# Load FAISS index\n",
    "# retriever.index = faiss.read_index('f1_facts.index')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
